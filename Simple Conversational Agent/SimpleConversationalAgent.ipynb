{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e477e0a6",
   "metadata": {},
   "source": [
    "## Building a Conversational Agent with Context Awareness with help of langchain   \n",
    "\n",
    "#### Overview\n",
    "In this notebook, I am going to build a simple conversational agent using langchain. that maintains context of the conversation and can answer questions based on the context across the conversation.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "1. Language Model - Core AI component to generate responses based on the input.\n",
    "2. Prompt Template - Defines the structure of the conversation and helps in maintaining context.\n",
    "3. Conversation Buffer Memory - Maintains the context of the conversation.\n",
    "4. Conversation Chain - Combines the language model, prompt template, and conversation buffer memory to generate responses.\n",
    "\n",
    "#### Pre-requisites\n",
    "1. Python 3.8 or higher\n",
    "2. LangChain\n",
    "3. OpenAI API Key\n",
    "4. Python-dotenv\n",
    "5. LangChain OpenAI\n",
    "6. Jupyter Notebook\n",
    "\n",
    "#### Setup\n",
    "1. Create a virtual environment and activate it.\n",
    "2. Create a chat history store\n",
    "3. Define the conversation Structure - \n",
    "    * A System message that defines the role of the agent.\n",
    "    * A Human message that defines the input of the agent.\n",
    "    * A Assistant message that defines the output of the agent.\n",
    "4. Build the conversation chain\n",
    "\n",
    "#### Usage\n",
    "1. Run the notebook.\n",
    "2. Enter your question.\n",
    "3. The agent will generate a response based on the input.\n",
    "4. The agent will maintain the context of the conversation and can answer questions based on the context across the conversation.\n",
    "\n",
    "#### Conclusion\n",
    "In this notebook, I have built a simple conversational agent using langchain. that maintains context of the conversation and can answer questions based on the context across the conversation.\n",
    "\n",
    "#### Enhancements\n",
    "- Will try to use open source models instead of paid models.\n",
    "- Will try to use other storage options instead of in memory storage.\n",
    "- Will try to use better prompt engineering to maintain context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee8b3bc",
   "metadata": {},
   "source": [
    "### Import required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92966cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q langchain langchain_experimental python-dotenv langchain_google_genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a86f6dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bda2fd",
   "metadata": {},
   "source": [
    "### Load environment variables and initialize the language model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db903a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyC4sO-oJ7g-vfpZXPchk-j7dE36nVGlIP4\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv('GOOGLE_API_KEY')\n",
    "print(os.getenv('GOOGLE_API_KEY'))\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\", # Changed model here!\n",
    "    temperature=0,\n",
    "    google_api_key=os.getenv('GOOGLE_API_KEY')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed3312f",
   "metadata": {},
   "source": [
    "#### Create a simple in-memory store for chat histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0deb3e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "def get_chat_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988f7234",
   "metadata": {},
   "source": [
    "#### Create the prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed3946ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cc3d02",
   "metadata": {},
   "source": [
    "#### Combine the prompt and model into a runnable chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a9a442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614a7d20",
   "metadata": {},
   "source": [
    "#### Wrap the chain with message history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a772d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_chat_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71d3eed",
   "metadata": {},
   "source": [
    "#### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d3999a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: I'm doing well, thank you for asking!  How are you today?\n",
      "AI: Your previous message was \"Hello! How are you?\"\n"
     ]
    }
   ],
   "source": [
    "session_id = \"user_123\"\n",
    "\n",
    "\n",
    "response1 = chain_with_history.invoke(\n",
    "    {\"input\": \"Hello! How are you?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"AI:\", response1.content)\n",
    "\n",
    "response2 = chain_with_history.invoke(\n",
    "    {\"input\": \"What was my previous message?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"AI:\", response2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b82d327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First interaction:\n",
      "AI: I'm doing well, thank you for asking!  How are you today?\n",
      "\n",
      "Second interaction (referencing previous message):\n",
      "AI: Your previous message was: \"Hello! How are you today?\"\n",
      "\n",
      "Third interaction with a different session (should not remember previous):\n",
      "AI: The capital of France is Paris.\n",
      "\n",
      "Fourth interaction with original session (should remember its own history):\n",
      "AI: As an AI, I have no memory of past conversations beyond our current one.  Our interaction started with your message \"Hello! How are you today?\".\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "session_id = \"user_321\" # A unique identifier for this conversation session\n",
    "\n",
    "print(\"First interaction:\")\n",
    "response1 = chain_with_history.invoke(\n",
    "    {\"input\": \"Hello! How are you today?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"AI:\", response1.content)\n",
    "\n",
    "print(\"\\nSecond interaction (referencing previous message):\")\n",
    "response2 = chain_with_history.invoke(\n",
    "    {\"input\": \"What was my previous message?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"AI:\", response2.content)\n",
    "\n",
    "print(\"\\nThird interaction with a different session (should not remember previous):\")\n",
    "session_id_2 = \"user_456\"\n",
    "response3 = chain_with_history.invoke(\n",
    "    {\"input\": \"What's the capital of France?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id_2}}\n",
    ")\n",
    "print(\"AI:\", response3.content)\n",
    "\n",
    "print(\"\\nFourth interaction with original session (should remember its own history):\")\n",
    "response4 = chain_with_history.invoke(\n",
    "    {\"input\": \"And what about my first message to you?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"AI:\", response4.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5b6410",
   "metadata": {},
   "source": [
    "#### Print the conversation history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bfde3f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversation History:\n",
      "human: Hello! How are you today?\n",
      "ai: I'm doing well, thank you for asking!  How are you today?\n",
      "human: What was my previous message?\n",
      "ai: Your previous message was: \"Hello! How are you today?\"\n",
      "human: And what about my first message to you?\n",
      "ai: As an AI, I have no memory of past conversations beyond our current one.  Our interaction started with your message \"Hello! How are you today?\".\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nConversation History:\")\n",
    "for message in store[session_id].messages:\n",
    "    print(f\"{message.type}: {message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258f0503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
