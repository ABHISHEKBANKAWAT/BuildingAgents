{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4625836e",
   "metadata": {},
   "source": [
    "## Scientific Paper Agent using LangGraph\n",
    "### Overview\n",
    "This project implements an intelligent research assistant that helps users navigate, understand, and analyze scientific literature using LangGraph and advanced language models. By combining various academic API with sophisticated paper processing techniques, it creates a seamless experience for researchers, students, and professionals working with academic papers.\n",
    "\n",
    "### Motivation \n",
    "Reduce the time people invest in R&D and reading, analyzing and synthesizing academic papers. \n",
    "Key Challenes include - \n",
    "* Extensive Time commitment \n",
    "* Inefficient search processes accross fragmented database ecosystems\n",
    "* Complex task synthesizingand connecting findings across multiple papers\n",
    "* Stay current with new publications\n",
    "\n",
    "## Key Components\n",
    "1. State-Driven Workflow Engine\n",
    "* StateGraph Architecture: Five-node system for orchestraded research.\n",
    "* Decision making node: Query intent analysis and routing.\n",
    "* Planning Node: Research Stategy formulation.\n",
    "* Tool Execution Node: Paper Retrievel and processing.\n",
    "* Judge Node: Quality validation and improvement cycles.\n",
    "\n",
    "2. Paper Processing Integration\n",
    "* Source Integration, CORE API for Comprehensive paper access.\n",
    "* Document Processing, PDF content extraction, Text Structure preservation.\n",
    "3. Analysis Workflow\n",
    "* State-aware processing pipeline\n",
    "* Multi-step validation gates\n",
    "* Quality-focused improvement cycles.\n",
    "* Human-in-the-loop validation options.\n",
    "\n",
    "\n",
    "### Method details\n",
    "#### The system requires\n",
    "\n",
    "- Gemini API Key.\n",
    "- CORE API key for paper retrieval. CORE is one of the larges online repositories for scientific papers, counting over 136 million papers, and offers a free API for personal use. A key can be requested here.\n",
    "#### Technical Architecture:\n",
    "\n",
    "- LangGraph for state orchestration.\n",
    "- PDFplumber for document processing.\n",
    "- Pydantic for structured data handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a613481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import os\n",
    "import urllib3\n",
    "import time\n",
    "\n",
    "import pdfplumber\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown\n",
    "from langchain_core.messages import BaseMessage, SystemMessage, ToolMessage, AIMessage\n",
    "from langchain_core.tools import BaseTool, tool\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Annotated, ClassVar, Sequence, TypedDict, Optional\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv('GOOGLE_API_KEY')\n",
    "os.environ[\"CORE_API_KEY\"] = os.getenv('CORE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb770d4",
   "metadata": {},
   "source": [
    "## Prompts\n",
    "this cell contains the prompts used in the workflow\n",
    "\n",
    "the agent_prompt  contains a section how to use complex queries with CORE API enabling agent to solve more complex tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ff595dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt for the initial decision making on how to reply to the user \n",
    "decision_making_prompt = \"\"\"\n",
    "You are an experienced scientific researcher.\n",
    "Your goal is to help the use with their scientific research.\n",
    "\n",
    "Based on the user query, decide if you need to perform a research or if you can answer the question directly.\n",
    "- You should perform a research if the user query requires any supporting evidence or information.\n",
    "- You should answer the question directly if the user query is a simple question that can be answered without any supporting evidence or information.\n",
    "\"\"\"\n",
    "\n",
    "# planning \n",
    "planning_prompt = \"\"\"\n",
    "You are an experienced scientific researcher.\n",
    "Your goal is to make a new step by step plan to help the user with their scientific research.\n",
    "\n",
    "Subtasks should not rely on any assumptions or guesses, but only rely on the information provided in the context or loop up for any additional information. \n",
    "if any feedback is provided about a previous answer, incorporate it into the new planning.\n",
    "# Tools\n",
    "\n",
    "for each substack, indicate the external tool required to complete the substack.\n",
    "Tools can be one of the following:\n",
    "{tools}\n",
    "\"\"\"\n",
    "\n",
    "# Prompt for the agent to answer the user query\n",
    "agent_prompt = \"\"\"\n",
    "#IDETITY AND PURPOSE\n",
    "\n",
    "You are an experienced scientific researcher.\n",
    "Your goal is to help the user with their scientific research. You have access to a set of external tools to complete your tasks.\n",
    "Follow the plan you wrote to successfully complete the task.\n",
    "Add extensive inline citations to support any claim made in the answer.\n",
    "\n",
    "\n",
    "# EXTERNAL KNOWLEDGE\n",
    "\n",
    "## CORE API\n",
    "\n",
    "The CORE API has a specific query language that allows you to explore a vast papers collection and perform complex queries. See the following table for a list of available operators:\n",
    "\n",
    "| Operator       | Accepted symbols         | Meaning                                                                                      |\n",
    "|---------------|-------------------------|----------------------------------------------------------------------------------------------|\n",
    "| And           | AND, +, space          | Logical binary and.                                                                           |\n",
    "| Or            | OR                     | Logical binary or.                                                                            |\n",
    "| Grouping      | (...)                  | Used to prioritise and group elements of the query.                                           |\n",
    "| Field lookup  | field_name:value       | Used to support lookup of specific fields.                                                    |\n",
    "| Range queries | fieldName(>, <,>=, <=) | For numeric and date fields, it allows to specify a range of valid values to return.         |\n",
    "| Exists queries| _exists_:fieldName     | Allows for complex queries, it returns all the items where the field specified by fieldName is not empty. |\n",
    "\n",
    "Use this table to formulate more complex queries filtering for specific papers, for example publication date/year.\n",
    "Here are the relevant fields of a paper object you can use to filter the results:\n",
    "{\n",
    "  \"authors\": [{\"name\": \"Last Name, First Name\"}],\n",
    "  \"documentType\": \"presentation\" or \"research\" or \"thesis\",\n",
    "  \"publishedDate\": \"2019-08-24T14:15:22Z\",\n",
    "  \"title\": \"Title of the paper\",\n",
    "  \"yearPublished\": \"2019\"\n",
    "}\n",
    "\n",
    "Example queries:\n",
    "- \"machine learning AND yearPublished:2023\"\n",
    "- \"maritime biology AND yearPublished>=2023 AND yearPublished<=2024\"\n",
    "- \"cancer research AND authors:Vaswani, Ashish AND authors:Bello, Irwan\"\n",
    "- \"title:Attention is all you need\"\n",
    "- \"mathematics AND _exists_:abstract\"\n",
    "\"\"\"\n",
    "\n",
    "# Prompt for the judging step to evaluate the quality of the final answer\n",
    "judge_prompt = \"\"\"\n",
    "You are an expert scientific researcher.\n",
    "Your goal is to review the final answer you provided for a specific user query.\n",
    "\n",
    "Look at the conversation history between you and the user. Based on it, you need to decide if the final answer is satisfactory or not.\n",
    "\n",
    "A good final answer should:\n",
    "- Directly answer the user query. For example, it does not answer a question about a different paper or area of research.\n",
    "- Answer extensively the request from the user.\n",
    "- Take into account any feedback given through the conversation.\n",
    "- Provide inline sources to support any claim made in the answer.\n",
    "\n",
    "In case the answer is not good enough, provide clear and concise feedback on what needs to be improved to pass the evaluation.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecade80d",
   "metadata": {},
   "source": [
    "## Utility classes and functions\n",
    "This cell contains the utility classes and functions used in the workflow. It includes a wrapper around the CORE API, the Pydantic models for the input and output of the nodes, and a few general-purpose functions.\n",
    "\n",
    "The CoreAPIWrapper class includes a retry mechanism to handle transient errors and make the workflow more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53d97493",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoreAPIWrapper(BaseModel):\n",
    "    \"\"\"Simple wrapper around CORE API\"\"\"\n",
    "    base_url: ClassVar[str] = \"https://api.core.ac.uk/v3\"\n",
    "    api_key: ClassVar[str] = os.getenv(\"CORE_API_KEY\")\n",
    "\n",
    "    top_k_results: int = Field(description=\"Top k results obtained by running a query on Core\", default=1)\n",
    "\n",
    "    def _get_search_response(self, query: str) -> dict:\n",
    "        http = urllib3.PoolManager()\n",
    "        # Retry mechanism to handle transient errors\n",
    "        max_retries = 5\n",
    "        for attempt in range(max_retries):\n",
    "            response = http.request(\n",
    "                'GET',\n",
    "                f\"{self.base_url}/search/outputs\",\n",
    "                headers={\"Authorization\": f\"Bearer {self.api_key}\"},\n",
    "                fields = {\"q\":query, \"limit\":self.top_k_results}\n",
    "                )\n",
    "            if 200 <= response.status < 300:\n",
    "                return response.json()\n",
    "            elif attempt < max_retrie-1:\n",
    "                time.sleep(2 ** (attempt+2))\n",
    "            else:\n",
    "                raise Exception(f\"Error: {response.status} {response.reason}\")\n",
    "\n",
    "    def search(self, query: str) -> str:\n",
    "        response = self._get_search_response(query)\n",
    "        results = response.get(\"results\", [])\n",
    "        if not results:\n",
    "            return \"No relevant results were found\"\n",
    "\n",
    "        # Format the results in a string\n",
    "        docs = []\n",
    "        for result in results:\n",
    "            published_date_str = result.get('publishedDate') or result.get('yearPublished', '')\n",
    "            authors_str = ' and '.join([item['name'] for item in result.get('authors', [])])\n",
    "            docs.append((\n",
    "                f\"* ID: {result.get('id', '')},\\n\"\n",
    "                f\"* Title: {result.get('title', '')},\\n\"\n",
    "                f\"* Published Date: {published_date_str},\\n\"\n",
    "                f\"* Authors: {authors_str},\\n\"\n",
    "                f\"* Abstract: {result.get('abstract', '')},\\n\"\n",
    "                f\"* Paper URLs: {result.get('sourceFulltextUrls') or result.get('downloadUrl', '')}\"\n",
    "            ))\n",
    "        return \"\\n-----\\n\".join(docs)    \n",
    "\n",
    "class SearchPapersInput(BaseModel):\n",
    "    \"\"\"Input object to search papers with the CORE API.\"\"\"\n",
    "    query: str = Field(description=\"The query to search for on the selected archive.\")\n",
    "    max_papers: int = Field(description=\"The maximum number of papers to return. It's default to 1, but you can increase it up to 10 in case you need to perform a more comprehensive search.\", default=1, ge=1, le=10)\n",
    "\n",
    "class DecisionMakingOutput(BaseModel):\n",
    "    \"\"\"Output object of the decision making node.\"\"\"\n",
    "    requires_research: bool = Field(description=\"Whether the user query requires research or not.\")\n",
    "    answer: Optional[str] = Field(default=None, description=\"The answer to the user query. It should be None if the user query requires research, otherwise it should be a direct answer to the user query.\")\n",
    "\n",
    "class JudgeOutput(BaseModel):\n",
    "    \"\"\"Output object of the judge node.\"\"\"\n",
    "    is_good_answer: bool = Field(description=\"Whether the answer is good or not.\")\n",
    "    feedback: Optional[str] = Field(default=None, description=\"Detailed feedback about why the answer is not good. It should be None if the answer is good.\")\n",
    "\n",
    "def format_tools_description(tools: list[BaseTool]) -> str:\n",
    "    return \"\\n\\n\".join([f\"- {tool.name}: {tool.description}\\n Input arguments: {tool.args}\" for tool in tools])\n",
    "\n",
    "async def print_stream(app: CompiledStateGraph, input: str) -> Optional[BaseMessage]:\n",
    "    display(Markdown(\"## New research running\"))\n",
    "    display(Markdown(f\"### Input:\\n\\n{input}\\n\\n\"))\n",
    "    display(Markdown(\"### Stream:\\n\\n\"))\n",
    "\n",
    "    # Stream the results \n",
    "    all_messages = []\n",
    "    async for chunk in app.astream({\"messages\": [input]}, stream_mode=\"updates\"):\n",
    "        for updates in chunk.values():\n",
    "            if messages := updates.get(\"messages\"):\n",
    "                all_messages.extend(messages)\n",
    "                for message in messages:\n",
    "                    message.pretty_print()\n",
    "                    print(\"\\n\\n\")\n",
    " \n",
    "    # Return the last message if any\n",
    "    if not all_messages:\n",
    "        return None\n",
    "    return all_messages[-1]\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6df709f",
   "metadata": {},
   "source": [
    "## Agent state\n",
    "This cell defines the agent state, which contains the following information:\n",
    "\n",
    "- requires_research: Whether the user query requires research or not.\n",
    "- num_feedback_requests: The number of times the LLM asked for feedback.\n",
    "- is_good_answer: Whether the LLM's final answer is good or not.\n",
    "- messages: The conversation history between the user and the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "572e4c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"The state of the agent during the paper research process.\"\"\"\n",
    "    requires_research: bool = False\n",
    "    num_feedback_requests: int = 0\n",
    "    is_good_answer: bool = False\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce25284",
   "metadata": {},
   "source": [
    "Agent tools\n",
    "This cell defines the tools available to the agent. The toolkit contains a tool to search for scientific papers using the CORE API, a tool to download a scientific paper from a given URL, and a tool to ask for human feedback.\n",
    "\n",
    "To make the paper download more robust, the tool includes a retry mechanism, similar to the one used for the CORE API, as well as a mock browser header to avoid 403 errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ddc4477",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"search-papers\", args_schema=SearchPapersInput)\n",
    "def search_papers(query: str, max_papers: int = 1) -> str:\n",
    "    \"\"\"Search for scientific papers using the CORE API.\n",
    "\n",
    "    Example:\n",
    "    {\"query\": \"Attention is all you need\", \"max_papers\": 1}\n",
    "\n",
    "    Returns:\n",
    "        A list of the relevant papers found with the corresponding relevant information.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return CoreAPIWrapper(top_k_results=max_papers).search(query)\n",
    "    except Exception as e:\n",
    "        return f\"Error performing paper search: {e}\"\n",
    "\n",
    "@tool(\"download-paper\")\n",
    "def download_paper(url: str) -> str:\n",
    "    \"\"\"Download a specific scientific paper from a given URL.\n",
    "\n",
    "    Example:\n",
    "    {\"url\": \"https://sample.pdf\"}\n",
    "\n",
    "    Returns:\n",
    "        The paper content.\n",
    "    \"\"\"\n",
    "    try:        \n",
    "        http = urllib3.PoolManager(\n",
    "            cert_reqs='CERT_NONE',\n",
    "        )\n",
    "        \n",
    "        # Mock browser headers to avoid 403 error\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Connection': 'keep-alive',\n",
    "        }\n",
    "        max_retries = 5\n",
    "        for attempt in range(max_retries):\n",
    "            response = http.request('GET', url, headers=headers)\n",
    "            if 200 <= response.status < 300:\n",
    "                pdf_file = io.BytesIO(response.data)\n",
    "                with pdfplumber.open(pdf_file) as pdf:\n",
    "                    text = \"\"\n",
    "                    for page in pdf.pages:\n",
    "                        text += page.extract_text() + \"\\n\"\n",
    "                return text\n",
    "            elif attempt < max_retries - 1:\n",
    "                time.sleep(2 ** (attempt + 2))\n",
    "            else:\n",
    "                raise Exception(f\"Got non 2xx when downloading paper: {response.status_code} {response.text}\")\n",
    "    except Exception as e:\n",
    "        return f\"Error downloading paper: {e}\"\n",
    "\n",
    "@tool(\"ask-human-feedback\")\n",
    "def ask_human_feedback(question: str) -> str:\n",
    "    \"\"\"Ask for human feedback. You should call this tool when encountering unexpected errors.\"\"\"\n",
    "    return input(question)\n",
    "\n",
    "tools = [search_papers, download_paper, ask_human_feedback]\n",
    "tools_dict = {tool.name: tool for tool in tools}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baed7422",
   "metadata": {},
   "source": [
    "Workflow nodes\n",
    "This cell defines the nodes of the workflow. Note how the judge_node is configured to end the execution if the LLM failed to provide a good answer twice to keep latency acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLMs\n",
    "base_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\", # Changed model here!\n",
    "    temperature=0,\n",
    "    google_api_key=os.getenv('GOOGLE_API_KEY')\n",
    ")\n",
    "decision_making_llm = base_llm.with_structured_output(DecisionMakingOutput)\n",
    "agent_llm = base_llm.bind_tools(tools)\n",
    "judge_llm = base_llm.with_structured_output(JudgeOutput)\n",
    "\n",
    "# Decision making node\n",
    "def decision_making_node(state: AgentState):\n",
    "    \"\"\"Entry point of the workflow. Based on the user query, the model can either respond directly or perform a full research, routing the workflow to the planning node\"\"\"\n",
    "    system_prompt = SystemMessage(content=decision_making_prompt)\n",
    "    response: DecisionMakingOutput = decision_making_llm.invoke([system_prompt] + state[\"messages\"])\n",
    "    output = {\"requires_research\": response.requires_research}\n",
    "    if response.answer:\n",
    "        output[\"messages\"] = [AIMessage(content=response.answer)]\n",
    "    return output\n",
    "\n",
    "# Task router function\n",
    "def router(state: AgentState):\n",
    "    \"\"\"Router directing the user query to the appropriate branch of the workflow.\"\"\"\n",
    "    if state[\"requires_research\"]:\n",
    "        return \"planning\"\n",
    "    else:\n",
    "        return \"end\"\n",
    "\n",
    "# Planning node\n",
    "def planning_node(state: AgentState):\n",
    "    \"\"\"Planning node that creates a step by step plan to answer the user query.\"\"\"\n",
    "    system_prompt = SystemMessage(content=planning_prompt.format(tools=format_tools_description(tools)))\n",
    "    response = base_llm.invoke([system_prompt] + state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Tool call node\n",
    "def tools_node(state: AgentState):\n",
    "    \"\"\"Tool call node that executes the tools based on the plan.\"\"\"\n",
    "    outputs = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool_result = tools_dict[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "        outputs.append(\n",
    "            ToolMessage(\n",
    "                content=json.dumps(tool_result),\n",
    "                name=tool_call[\"name\"],\n",
    "                tool_call_id=tool_call[\"id\"],\n",
    "            )\n",
    "        )\n",
    "    return {\"messages\": outputs}\n",
    "\n",
    "# Agent call node\n",
    "def agent_node(state: AgentState):\n",
    "    \"\"\"Agent call node that uses the LLM with tools to answer the user query.\"\"\"\n",
    "    system_prompt = SystemMessage(content=agent_prompt)\n",
    "    response = agent_llm.invoke([system_prompt] + state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Should continue function\n",
    "def should_continue(state: AgentState):\n",
    "    \"\"\"Check if the agent should continue or end.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    # End execution if there are no tool calls\n",
    "    if last_message.tool_calls:\n",
    "        return \"continue\"\n",
    "    else:\n",
    "        return \"end\"\n",
    "\n",
    "# Judge node\n",
    "def judge_node(state: AgentState):\n",
    "    \"\"\"Node to let the LLM judge the quality of its own final answer.\"\"\"\n",
    "    # End execution if the LLM failed to provide a good answer twice.\n",
    "    num_feedback_requests = state.get(\"num_feedback_requests\", 0)\n",
    "    if num_feedback_requests >= 2:\n",
    "        return {\"is_good_answer\": True}\n",
    "\n",
    "    system_prompt = SystemMessage(content=judge_prompt)\n",
    "    response: JudgeOutput = judge_llm.invoke([system_prompt] + state[\"messages\"])\n",
    "    output = {\n",
    "        \"is_good_answer\": response.is_good_answer,\n",
    "        \"num_feedback_requests\": num_feedback_requests + 1\n",
    "    }\n",
    "    if response.feedback:\n",
    "        output[\"messages\"] = [AIMessage(content=response.feedback)]\n",
    "    return output\n",
    "\n",
    "# Final answer router function\n",
    "def final_answer_router(state: AgentState):\n",
    "    \"\"\"Router to end the workflow or improve the answer.\"\"\"\n",
    "    if state[\"is_good_answer\"]:\n",
    "        return \"end\"\n",
    "    else:\n",
    "        return \"planning\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b08712",
   "metadata": {},
   "source": [
    "Workflow definition\n",
    "This cell defines the workflow using LangGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8632691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StateGraph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes to the graph\n",
    "workflow.add_node(\"decision_making\", decision_making_node)\n",
    "workflow.add_node(\"planning\", planning_node)\n",
    "workflow.add_node(\"tools\", tools_node)\n",
    "workflow.add_node(\"agent\", agent_node)\n",
    "workflow.add_node(\"judge\", judge_node)\n",
    "\n",
    "# Set the entry point of the graph\n",
    "workflow.set_entry_point(\"decision_making\")\n",
    "\n",
    "# Add edges between nodes\n",
    "workflow.add_conditional_edges(\n",
    "    \"decision_making\",\n",
    "    router,\n",
    "    {\n",
    "        \"planning\": \"planning\",\n",
    "        \"end\": END,\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"planning\", \"agent\")\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"tools\",\n",
    "        \"end\": \"judge\",\n",
    "    },\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"judge\",\n",
    "    final_answer_router,\n",
    "    {\n",
    "        \"planning\": \"planning\",\n",
    "        \"end\": END,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acf0253",
   "metadata": {},
   "source": [
    "### Example usecase for PhD academic research\n",
    "This cell tests the workflow with several example queries. These queries are designed to evaluate the agent on the following aspects:\n",
    "\n",
    "- Completing tasks that are representative of the work a PhD researcher might need to perform.\n",
    "- Addressing more specific tasks that require researching papers within a defined timeframe.\n",
    "- Tackling tasks across multiple areas of research.\n",
    "- Critically evaluating its own responses by sourcing specific information from the papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3f76f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## New research running"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Input:\n",
       "\n",
       "Download and summarize the findings of this paper: https://pmc.ncbi.nlm.nih.gov/articles/PMC11379842/pdf/11671_2024_Article_4070.pdf\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Stream:\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here's a step-by-step plan to download and summarize the findings of the provided paper.  This plan avoids assumptions and relies solely on the provided URL and available tools.\n",
      "\n",
      "\n",
      "**Step 1: Download the Paper**\n",
      "\n",
      "* **Tool:** `download-paper`\n",
      "* **Input:**\n",
      "    ```json\n",
      "    {\"url\": \"https://pmc.ncbi.nlm.nih.gov/articles/PMC11379842/pdf/11671_2024_Article_4070.pdf\"}\n",
      "    ```\n",
      "* **Output:** The full text content of the paper as a string (let's call this `paper_content`).  This step might fail if the URL is incorrect or the paper is not accessible.  Error handling is crucial here.\n",
      "\n",
      "\n",
      "**Step 2:  Handle Potential Download Errors**\n",
      "\n",
      "* **Tool:** `ask-human-feedback` (conditional)\n",
      "* **Condition:** If Step 1 fails (e.g., returns an error), proceed to this step.\n",
      "* **Input:**\n",
      "    ```json\n",
      "    {\n",
      "      \"question\": \"The download of the paper from 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11379842/pdf/11671_2024_Article_4070.pdf' failed. Please check the URL and provide the correct one or indicate if the paper is inaccessible.\"\n",
      "    }\n",
      "    ```\n",
      "* **Output:**  A corrected URL or confirmation of inaccessibility.  If a corrected URL is provided, return to Step 1 with the new URL. If the paper is inaccessible, the process stops.\n",
      "\n",
      "\n",
      "**Step 3: Extract Key Findings (Requires Advanced NLP)**\n",
      "\n",
      "This step requires a more sophisticated tool than what's currently available.  A robust solution would involve a Natural Language Processing (NLP) model capable of summarizing scientific papers.  Since we don't have such a tool, we'll outline a simplified approach that relies on human intervention for now.  A future iteration could incorporate a more advanced NLP tool.\n",
      "\n",
      "* **Tool:**  `ask-human-feedback` (or a future NLP tool)\n",
      "* **Input (for human feedback):**\n",
      "    ```json\n",
      "    {\n",
      "      \"question\": \"Please summarize the key findings of the following paper content:\\n\\n\" + `paper_content`\n",
      "    }\n",
      "    ```\n",
      "* **Output:** A human-generated summary of the paper's key findings.\n",
      "\n",
      "\n",
      "**Step 4:  (Future Improvement) Automated Summarization**\n",
      "\n",
      "This step would be replaced in a future version with an advanced NLP tool capable of automatically summarizing scientific papers.  The tool would take `paper_content` as input and return a summary.\n",
      "\n",
      "\n",
      "**Error Handling Summary:**\n",
      "\n",
      "The plan includes explicit error handling for the download step.  The use of `ask-human-feedback` allows for human intervention when unexpected issues arise, ensuring robustness.  The plan also acknowledges the limitations of current tools for automated summarization and proposes a path for future improvement.\n",
      "\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "**Python Code (Illustrative - Requires NLP Tool for Step 3):**\n",
      "\n",
      "```python\n",
      "paper_url = \"https://pmc.ncbi.nlm.nih.gov/articles/PMC11379842/pdf/11671_2024_Article_4070.pdf\"\n",
      "\n",
      "try:\n",
      "    paper_content = default_api.download_paper(url=paper_url)\n",
      "    # Step 3:  (Requires an NLP summarization tool - Placeholder)\n",
      "    summary = default_api.ask_human_feedback(question=f\"Please summarize the key findings of this paper:\\n\\n{paper_content}\")\n",
      "    print(f\"Summary:\\n{summary}\")\n",
      "\n",
      "except Exception as e:\n",
      "    print(f\"An error occurred: {e}\")\n",
      "    feedback = default_api.ask_human_feedback(question=f\"Download of {paper_url} failed. Please provide a valid URL or indicate if the paper is inaccessible.\")\n",
      "    print(f\"Human feedback: {feedback}\")\n",
      "\n",
      "```\n",
      "\n",
      "This code provides a framework.  The crucial Step 3 (summarization) requires a more advanced NLP capability to be fully automated.\n",
      "\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The response provides a good plan for downloading and summarizing the paper, including error handling. However, it lacks the actual implementation of the summarization step, which is the core of the user's request.  The code is illustrative and requires a sophisticated NLP tool to be functional.  The answer should include a functional summarization, even if it's a simplified one, or clearly state the limitations and the need for advanced tools.\n",
      "\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here's an improved plan that addresses these points:\n",
      "\n",
      "\n",
      "**Improved Step-by-Step Plan:**\n",
      "\n",
      "**Step 1: Download the Paper**\n",
      "\n",
      "* **Tool:** `download-paper`\n",
      "* **Input:**  `{\"url\": \"https://pmc.ncbi.nlm.nih.gov/articles/PMC11379842/pdf/11671_2024_Article_4070.pdf\"}`\n",
      "* **Output:**  `paper_content` (string containing the paper's text) or an error.\n",
      "\n",
      "**Step 2: Error Handling for Download**\n",
      "\n",
      "* **Tool:** `ask-human-feedback` (conditional, if Step 1 fails)\n",
      "* **Input:**  A message requesting a corrected URL or confirmation of inaccessibility.\n",
      "* **Output:**  Corrected URL or confirmation of inaccessibility.  If a corrected URL is provided, return to Step 1.\n",
      "\n",
      "**Step 3:  Simplified Summarization (using available tools)**\n",
      "\n",
      "Since a sophisticated NLP summarization tool is not available, we'll use a simplified approach.  This will involve extracting key sections (Abstract, Conclusion, etc.) and concatenating them for a rudimentary summary.  This is not ideal but is feasible with the given tools.\n",
      "\n",
      "* **Tool:**  (No external tool needed for this simplified approach. String manipulation will be done within the code.)\n",
      "* **Process:**\n",
      "    1.  Identify sections like \"Abstract,\" \"Introduction,\" \"Results,\" \"Discussion,\" and \"Conclusion\" within `paper_content`.\n",
      "    2.  Extract the text content of these sections.\n",
      "    3.  Concatenate the extracted text to create a simplified summary.\n",
      "\n",
      "**Step 4:  Present the Summary**\n",
      "\n",
      "* **Tool:**  (No external tool needed.  Output is printed to the console.)\n",
      "* **Process:** Print the simplified summary generated in Step 3.\n",
      "\n",
      "\n",
      "**Python Code (with simplified summarization):**\n",
      "\n",
      "```python\n",
      "import re\n",
      "\n",
      "paper_url = \"https://pmc.ncbi.nlm.nih.gov/articles/PMC11379842/pdf/11671_2024_Article_4070.pdf\"\n",
      "\n",
      "try:\n",
      "    paper_content = default_api.download_paper(url=paper_url)  # Placeholder for download function\n",
      "\n",
      "    # Simplified Summarization\n",
      "    sections = [\"Abstract\", \"Introduction\", \"Results\", \"Discussion\", \"Conclusion\"]\n",
      "    summary = \"\"\n",
      "    for section in sections:\n",
      "        match = re.search(rf\"{section}.*?(\\n|$)\", paper_content, re.IGNORECASE | re.DOTALL)\n",
      "        if match:\n",
      "            summary += match.group(0).strip() + \"\\n\\n\"\n",
      "\n",
      "    print(f\"Simplified Summary:\\n{summary}\")\n",
      "\n",
      "except Exception as e:\n",
      "    print(f\"An error occurred: {e}\")\n",
      "    feedback = default_api.ask_human_feedback(question=f\"Download of {paper_url} failed. Please provide a valid URL or indicate if the paper is inaccessible.\")\n",
      "    print(f\"Human feedback: {feedback}\")\n",
      "\n",
      "```\n",
      "\n",
      "This improved plan provides a more complete solution, acknowledging the limitations of available tools while offering a functional, albeit simplified, summarization approach.  The code uses regular expressions for a basic section extraction, which can be improved with more sophisticated NLP techniques in the future.\n",
      "\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## New research running"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Input:\n",
       "\n",
       "Can you find 8 papers on quantum machine learning?\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Stream:\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here's a step-by-step plan to find 8 papers on quantum machine learning, using the provided tools.  This plan accounts for potential issues and incorporates human feedback where necessary.\n",
      "\n",
      "\n",
      "**Step 1:  Initial Search**\n",
      "\n",
      "* **Tool:** `search-papers`\n",
      "* **Input:**  `{\"query\": \"quantum machine learning\", \"max_papers\": 8}`\n",
      "* **Expected Output:** A list of up to 8 papers on quantum machine learning, including titles, authors, and URLs.\n",
      "* **Error Handling:** If fewer than 8 papers are returned, proceed to Step 2. If the API returns an error, proceed to Step 3.\n",
      "\n",
      "\n",
      "**Step 2: Broaden the Search (if necessary)**\n",
      "\n",
      "* **Condition:** Fewer than 8 papers were returned in Step 1.\n",
      "* **Tool:** `search-papers`\n",
      "* **Input:**  `{\"query\": \"quantum machine learning OR quantum computing machine learning OR quantum algorithms machine learning\", \"max_papers\": 8}`  This expands the search terms to capture a wider range of relevant papers.\n",
      "* **Expected Output:** A list of up to 8 papers.\n",
      "* **Error Handling:** If fewer than 8 papers are returned after this broader search, proceed to Step 4. If the API returns an error, proceed to Step 3.\n",
      "\n",
      "\n",
      "**Step 3: Seek Human Assistance (for API errors)**\n",
      "\n",
      "* **Condition:** The `search-papers` tool returned an error in either Step 1 or Step 2.\n",
      "* **Tool:** `ask-human-feedback`\n",
      "* **Input:** `{\"question\": \"The search-papers tool returned an error with the query 'quantum machine learning'.  Can you suggest alternative search terms or identify potential issues with the API?\"}`\n",
      "* **Expected Output:**  Human feedback providing alternative search terms or troubleshooting advice for the API.  This feedback will inform the next iteration of the search.\n",
      "\n",
      "\n",
      "**Step 4: Targeted Search (if necessary)**\n",
      "\n",
      "* **Condition:** Fewer than 8 papers were returned after Step 2.\n",
      "* **Action:**  Analyze the titles and abstracts of the papers already retrieved to identify key subtopics within quantum machine learning (e.g., quantum neural networks, quantum support vector machines).\n",
      "* **Tool:** `search-papers`\n",
      "* **Input:**  Multiple calls to `search-papers` using more specific queries based on the subtopics identified in the previous step. For example: `{\"query\": \"quantum neural networks\", \"max_papers\": 3}` and `{\"query\": \"quantum support vector machines\", \"max_papers\": 5}`.  Adjust the `max_papers` value for each query to reach a total of 8 papers.\n",
      "* **Expected Output:** A list of up to 8 papers, filling the gap from previous searches.\n",
      "* **Error Handling:** If the API returns an error, go back to Step 3.\n",
      "\n",
      "\n",
      "**Step 5: Download Papers (Optional)**\n",
      "\n",
      "* **Condition:**  The user wants to download the full text of the papers.\n",
      "* **Tool:** `download-paper`\n",
      "* **Input:** Multiple calls to `download-paper`, one for each paper URL obtained in the previous steps.\n",
      "* **Expected Output:** The full text content of each paper.\n",
      "* **Error Handling:** If a download fails, try again after a short delay. If repeated attempts fail, proceed to Step 3 to seek human assistance.\n",
      "\n",
      "\n",
      "This plan systematically addresses the task, handles potential errors, and leverages human feedback when necessary to ensure a successful outcome.  The use of multiple search queries with progressively more specific keywords increases the likelihood of finding a sufficient number of relevant papers.\n",
      "\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  search-papers (47e35d02-4a73-4131-814a-28421482c822)\n",
      " Call ID: 47e35d02-4a73-4131-814a-28421482c822\n",
      "  Args:\n",
      "    max_papers: 8.0\n",
      "    query: quantum machine learning\n",
      "\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: search-papers\n",
      "\n",
      "\"* ID: 73423988,\\n* Title: Quantum Machine Learning,\\n* Published Date: 2018-05-10T01:00:00+01:00,\\n* Authors: Biamonte, Jacob and Wittek, Peter and Pancotti, Nicola and Rebentrost, Patrick and Wiebe, Nathan and Lloyd, Seth,\\n* Abstract: Fuelled by increasing computer power and algorithmic advances, machine\\nlearning techniques have become powerful tools for finding patterns in data.\\nSince quantum systems produce counter-intuitive patterns believed not to be\\nefficiently produced by classical systems, it is reasonable to postulate that\\nquantum computers may outperform classical computers on machine learning tasks.\\nThe field of quantum machine learning explores how to devise and implement\\nconcrete quantum software that offers such advantages. Recent work has made\\nclear that the hardware and software challenges are still considerable but has\\nalso opened paths towards solutions.Comment: 24 pages, 2 figure,\\n* Paper URLs: ['http://arxiv.org/abs/1611.09347']\\n-----\\n* ID: 129348586,\\n* Title: Machine Learning by Unitary Tensor Network of Hierarchical Tree\\n  Structure,\\n* Published Date: 2019-03-10T00:00:00+00:00,\\n* Authors: Liu, Ding and Ran, Shi-Ju and Wittek, Peter and Peng, Cheng and Garc\\u00eda, Raul Bl\\u00e1zquez and Su, Gang and Lewenstein, Maciej,\\n* Abstract: The resemblance between the methods used in quantum-many body physics and in\\nmachine learning has drawn considerable attention. In particular, tensor\\nnetworks (TNs) and deep learning architectures bear striking similarities to\\nthe extent that TNs can be used for machine learning. Previous results used\\none-dimensional TNs in image recognition, showing limited scalability and\\nflexibilities. In this work, we train two-dimensional hierarchical TNs to solve\\nimage recognition problems, using a training algorithm derived from the\\nmulti-scale entanglement renormalization ansatz. This approach introduces\\nmathematical connections among quantum many-body physics, quantum information\\ntheory, and machine learning. While keeping the TN unitary in the training\\nphase, TN states are defined, which encode classes of images into quantum\\nmany-body states. We study the quantum features of the TN states, including\\nquantum entanglement and fidelity. We find these quantities could be properties\\nthat characterize the image classes, as well as the machine learning tasks.Comment: 6 pages, 4 figure,\\n* Paper URLs: ['http://arxiv.org/abs/1710.04833']\\n-----\\n* ID: 29557092,\\n* Title: Blind quantum machine learning,\\n* Published Date: 2015-07-26T01:00:00+01:00,\\n* Authors: Sheng, Yu-Bo and Zhou, Lan,\\n* Abstract: Blind quantum machine learning (BQML) enables a classical client with little\\nquantum technology to delegate a remote quantum machine learning to the quantum\\nserver in such a approach that the privacy data is preserved. Here we propose\\nthe first BQML protocol that the client can classify two-dimensional vectors to\\ndifferent clusters, resorting to a remote small-scale photon quantum\\ncomputation processor. During the protocol, the client is only required to\\nrotate and measure the single qubit. The protocol is secure without leaking any\\nrelevant information to the Eve. Any eavesdropper who attempts to intercept and\\ndisturb the learning process can be noticed. In principle, this protocol can be\\nused to classify high dimensional vectors and may provide a new viewpoint and\\napplication for quantum machine learning.Comment: 5 pages, 1 figure,\\n* Paper URLs: ['http://arxiv.org/abs/1507.07195']\\n-----\\n* ID: 141534527,\\n* Title: Experimental Machine Learning of Quantum States,\\n* Published Date: 2018-06-21T01:00:00+01:00,\\n* Authors: Gao, Jun and Qiao, Lu-Feng and Jiao, Zhi-Qiang and Ma, Yue-Chi and Hu, Cheng-Qiu and Ren, Ruo-Jing and Yang, Ai-Lin and Tang, Hao and Yung, Man-Hong and Jin, Xian-Min,\\n* Abstract: Quantum information technologies provide promising applications in\\ncommunication and computation, while machine learning has become a powerful\\ntechnique for extracting meaningful structures in 'big data'. A crossover\\nbetween quantum information and machine learning represents a new\\ninterdisciplinary area stimulating progresses in both fields. Traditionally, a\\nquantum state is characterized by quantum state tomography, which is a\\nresource-consuming process when scaled up. Here we experimentally demonstrate a\\nmachine-learning approach to construct a quantum-state classifier for\\nidentifying the separability of quantum states. We show that it is possible to\\nexperimentally train an artificial neural network to efficiently learn and\\nclassify quantum states, without the need of obtaining the full information of\\nthe states. We also show how adding a hidden layer of neurons to the neural\\nnetwork can significantly boost the performance of the state classifier. These\\nresults shed new light on how classification of quantum states can be achieved\\nwith limited resources, and represent a step towards machine-learning-based\\napplications in quantum information processing.Comment: 7 pages, 6 figure,\\n* Paper URLs: ['http://arxiv.org/abs/1712.00456']\\n-----\\n* ID: 25057605,\\n* Title: An introduction to quantum machine learning,\\n* Published Date: 2014-09-10T01:00:00+01:00,\\n* Authors: Schuld, M. and Sinayskiy, I. and Petruccione, F.,\\n* Abstract: Machine learning algorithms learn a desired input-output relation from\\nexamples in order to interpret new inputs. This is important for tasks such as\\nimage and speech recognition or strategy optimisation, with growing\\napplications in the IT industry. In the last couple of years, researchers\\ninvestigated if quantum computing can help to improve classical machine\\nlearning algorithms. Ideas range from running computationally costly algorithms\\nor their subroutines efficiently on a quantum computer to the translation of\\nstochastic methods into the language of quantum theory. This contribution gives\\na systematic overview of the emerging field of quantum machine learning. It\\npresents the approaches as well as technical details in an accessable way, and\\ndiscusses the potential of a future theory of quantum learning.Comment: to appear in Contemporary Physics; 19 pages, 10 figure,\\n* Paper URLs: ['http://arxiv.org/abs/1409.3097']\\n-----\\n* ID: 154985574,\\n* Title: Smooth input preparation for quantum and quantum-inspired machine\\n  learning,\\n* Published Date: 2019-08-02T01:00:00+01:00,\\n* Authors: Zhao, Zhikuan and Fitzsimons, Jack K. and Rebentrost, Patrick and Dunjko, Vedran and Fitzsimons, Joseph F.,\\n* Abstract: Machine learning has recently emerged as a fruitful area for finding\\npotential quantum computational advantage. Many of the quantum enhanced machine\\nlearning algorithms critically hinge upon the ability to efficiently produce\\nstates proportional to high-dimensional data points stored in a quantum\\naccessible memory. Even given query access to exponentially many entries stored\\nin a database, the construction of which is considered a one-off overhead, it\\nhas been argued that the cost of preparing such amplitude-encoded states may\\noffset any exponential quantum advantage. Here we prove using smoothed\\nanalysis, that if the data-analysis algorithm is robust against small\\nentry-wise input perturbation, state preparation can always be achieved with\\nconstant queries. This criterion is typically satisfied in realistic machine\\nlearning applications, where input data is subjective to moderate noise. Our\\nresults are equally applicable to the recent seminal progress in\\nquantum-inspired algorithms, where specially constructed databases suffice for\\npolylogarithmic classical algorithm in low-rank cases. The consequence of our\\nfinding is that for the purpose of practical machine learning, polylogarithmic\\nprocessing time is possible under a general and flexible input model with\\nquantum algorithms or quantum-inspired classical algorithms in the low-rank\\ncases.Comment: 5 pages, 1 figur,\\n* Paper URLs: ['http://arxiv.org/abs/1804.00281']\\n-----\\n* ID: 25064237,\\n* Title: Experimental Realization of Quantum Artificial Intelligence,\\n* Published Date: 2014-10-04T01:00:00+01:00,\\n* Authors: Zhaokai, Li and Xiaomei, Liu and Nanyang, Xu and jiangfeng, Du,\\n* Abstract: Machines are possible to have some artificial intelligence like human beings\\nowing to particular algorithms or software. Such machines could learn knowledge\\nfrom what people taught them and do works according to the knowledge. In\\npractical learning cases, the data is often extremely complicated and large,\\nthus classical learning machines often need huge computational resources.\\nQuantum machine learning algorithm, on the other hand, could be exponentially\\nfaster than classical machines using quantum parallelism. Here, we demonstrate\\na quantum machine learning algorithm on a four-qubit NMR test bench to solve an\\noptical character recognition problem, also known as the handwriting\\nrecognition. The quantum machine learns standard character fonts and then\\nrecognize handwritten characters from a set with two candidates. To our best\\nknowledge, this is the first artificial intelligence realized on a quantum\\nprocessor. Due to the widespreading importance of artificial intelligence and\\nits tremendous consuming of computational resources, quantum speedup would be\\nextremely attractive against the challenges from the Big Data.Comment: 7 pages, 4 figure,\\n* Paper URLs: ['http://arxiv.org/abs/1410.1054']\\n-----\\n* ID: 200818797,\\n* Title: NetKet: A Machine Learning Toolkit for Many-Body Quantum Systems,\\n* Published Date: 2019-03-29T00:00:00+00:00,\\n* Authors: Carleo, Giuseppe and Choo, Kenny and Hofmann, Damian and Smith, James E. T. and Westerhout, Tom and Alet, Fabien and Davis, Emily J. and Efthymiou, Stavros and Glasser, Ivan and Lin, Sheng-Hsuan and Mauri, Marta and Mazzola, Guglielmo and Mendl, Christian B. and van Nieuwenburg, Evert and O'Reilly, Ossian and Th\\u00e9veniaut, Hugo and Torlai, Giacomo and Wietek, Alexander,\\n* Abstract: We introduce NetKet, a comprehensive open source framework for the study of\\nmany-body quantum systems using machine learning techniques. The framework is\\nbuilt around a general and flexible implementation of neural-network quantum\\nstates, which are used as a variational ansatz for quantum wave functions.\\nNetKet provides algorithms for several key tasks in quantum many-body physics\\nand quantum technology, namely quantum state tomography, supervised learning\\nfrom wave-function data, and ground state searches for a wide range of\\ncustomizable lattice models. Our aim is to provide a common platform for open\\nresearch and to stimulate the collaborative development of computational\\nmethods at the interface of machine learning and many-body physics,\\n* Paper URLs: ['http://arxiv.org/abs/1904.00031']\"\n",
      "\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I found 8 papers on quantum machine learning.  The papers cover various aspects of the field, including theoretical foundations, experimental implementations, and applications.  The titles and abstracts provide a good overview of the research.  If you would like to download the full text of any of these papers, please provide the URLs.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## New research running"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Input:\n",
       "\n",
       "Find recent papers (2023-2024) about CRISPR applications in treating genetic disorders, \n",
       "    focusing on clinical trials and safety protocols\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Stream:\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here's a step-by-step plan to find recent papers (2023-2024) about CRISPR applications in treating genetic disorders, focusing on clinical trials and safety protocols.  This plan minimizes assumptions and relies on the specified tools.\n",
      "\n",
      "\n",
      "**Step 1: Search for relevant papers using the `search-papers` tool.**\n",
      "\n",
      "* **Goal:** Identify relevant papers published between 2023 and 2024.\n",
      "* **Tool:** `search-papers`\n",
      "* **Input:**\n",
      "```json\n",
      "{\n",
      "  \"query\": \"CRISPR AND (\\\"genetic disorders\\\" OR \\\"gene editing\\\") AND (\\\"clinical trial\\\" OR \\\"clinical trials\\\" OR \\\"safety protocol\\\" OR \\\"safety protocols\\\") AND (2023 OR 2024)\",\n",
      "  \"max_papers\": 10 \n",
      "}\n",
      "```\n",
      "* **Output:** A list of papers (up to 10) matching the query.  The output will include metadata such as title, authors, publication date, and potentially a URL.\n",
      "\n",
      "\n",
      "**Step 2:  Iterative refinement (if necessary).**\n",
      "\n",
      "* **Goal:** Improve the search results if Step 1 yields insufficient or irrelevant papers.\n",
      "* **Tool:** `search-papers` (repeatedly) and potentially `ask-human-feedback`.\n",
      "* **Process:**\n",
      "    1. **Analyze Step 1 results:** Examine the titles and abstracts of the returned papers.  Are they relevant? Are there any keywords missing from the initial query?\n",
      "    2. **Refine the query:** Based on the analysis, modify the query in the `search-papers` input.  For example, you might add more specific keywords related to particular genetic disorders (e.g., \"sickle cell anemia,\" \"beta-thalassemia\"), types of CRISPR systems (e.g., \"Cas9,\" \"Cas12a\"), or specific safety concerns (e.g., \"off-target effects,\" \"immunogenicity\").\n",
      "    3. **Repeat Step 1:** Run the refined query using `search-papers`.\n",
      "    4. **Human feedback (optional):** If after several iterations, the results are still unsatisfactory, use `ask-human-feedback` with a question like:  \"My search for papers on CRISPR clinical trials and safety protocols for genetic disorders is yielding poor results. My current query is [insert query]. Can you suggest improvements or alternative search terms?\"\n",
      "\n",
      "\n",
      "**Step 3: Download relevant papers (if URLs are available).**\n",
      "\n",
      "* **Goal:** Obtain the full text of the relevant papers identified in Step 1 or Step 2.\n",
      "* **Tool:** `download-paper`\n",
      "* **Input:** For each paper with a downloadable URL from the `search-papers` output, use the URL as input to `download-paper`.\n",
      "* **Output:** The full text content of each downloaded paper.\n",
      "\n",
      "\n",
      "**Step 4: Analyze the downloaded papers.**\n",
      "\n",
      "* **Goal:** Extract information on clinical trials and safety protocols from the downloaded papers.  This step is not automated and requires manual review.\n",
      "* **Tool:** None (manual analysis).\n",
      "* **Process:** Read the papers and extract relevant information, such as the specific genetic disorders targeted, the design of the clinical trials, the safety protocols implemented, and the results obtained.  Organize this information in a structured format (e.g., a table or spreadsheet).\n",
      "\n",
      "\n",
      "This plan provides a systematic approach to finding and analyzing the required papers. The iterative refinement in Step 2 ensures that the search is optimized for the specific needs of the research.  The use of `ask-human-feedback` provides a mechanism to overcome unexpected challenges.\n",
      "\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "```python\n",
      "print(default_api.search_papers(query='CRISPR AND (\"genetic disorders\" OR \"gene editing\") AND (\"clinical trial\" OR \"clinical trials\" OR \"safety protocol\" OR \"safety protocols\") AND (2023 OR 2024)', max_papers=10))\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The response is a plan, not an answer.  It does not actually retrieve or process any papers.  It needs to use the hypothetical APIs to get and display results.\n",
      "\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "To make this a functional plan, we need to replace the hypothetical API calls with actual code that interacts with real APIs (if such APIs exist and are accessible).  Since I don't have access to external APIs or the ability to execute code, I can only provide a *pseudo-code* example demonstrating how the plan could be implemented.  This example assumes the existence of Python libraries that interact with the hypothetical `search-papers` and `download-paper` tools.\n",
      "\n",
      "\n",
      "```python\n",
      "# Placeholder for hypothetical API interaction libraries\n",
      "# Replace these with actual libraries if available\n",
      "import hypothetical_search_papers_api as search\n",
      "import hypothetical_download_paper_api as download\n",
      "\n",
      "def find_crispr_papers():\n",
      "    \"\"\"Finds and processes papers on CRISPR applications.\"\"\"\n",
      "\n",
      "    query = \"CRISPR AND (\\\"genetic disorders\\\" OR \\\"gene editing\\\") AND (\\\"clinical trial\\\" OR \\\"clinical trials\\\" OR \\\"safety protocol\\\" OR \\\"safety protocols\\\") AND (2023 OR 2024)\"\n",
      "    max_papers = 10\n",
      "\n",
      "    try:\n",
      "        papers = search.search(query, max_papers)  # Hypothetical API call\n",
      "        print(f\"Found {len(papers)} papers.\")\n",
      "\n",
      "        relevant_papers = []\n",
      "        for paper in papers:\n",
      "            if \"url\" in paper and paper[\"url\"]: #Check if a URL is available\n",
      "                try:\n",
      "                    paper_content = download.download(paper[\"url\"]) #Hypothetical API call\n",
      "                    relevant_papers.append({\"title\": paper.get(\"title\", \"Untitled\"), \"content\": paper_content})\n",
      "                except Exception as e:\n",
      "                    print(f\"Error downloading paper {paper.get('title', 'N/A')}: {e}\")\n",
      "\n",
      "        # Step 4: Manual analysis (not automated here)\n",
      "        print(\"\\nDownloaded papers (requires manual analysis):\")\n",
      "        for paper in relevant_papers:\n",
      "            print(f\"Title: {paper['title']}\")\n",
      "            #print(f\"Content (excerpt): {paper['content'][:200]}...\") #Show a snippet\n",
      "\n",
      "    except Exception as e:\n",
      "        print(f\"An error occurred: {e}\")\n",
      "        # Consider adding ask_human_feedback here if appropriate\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    find_crispr_papers()\n",
      "\n",
      "```\n",
      "\n",
      "This pseudo-code outlines the structure.  To make it functional, you would need to replace the placeholder API calls with actual code using real APIs (if they exist and are accessible to you).  You would also need to implement the manual analysis step (Step 4) separately.  Remember to handle potential errors appropriately (e.g., network issues, API rate limits).\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 1\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The response is a plan, not an answer.  It does not actually retrieve or process any papers.  It needs to use the hypothetical APIs to get and display results.\n",
      "\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I apologize for the misunderstanding.  My previous responses were outlining a plan and providing pseudo-code due to the lack of access to real APIs within this environment.  I cannot directly execute code or interact with external resources.\n",
      "\n",
      "To get actual results, you would need to:\n",
      "\n",
      "1. **Identify suitable APIs:** Find APIs that provide access to scientific literature databases (like PubMed, Scopus, etc.).  These APIs usually have their own query languages and rate limits.\n",
      "\n",
      "2. **Obtain API keys:**  Most APIs require authentication using API keys.  You'll need to register for an account and obtain the necessary keys.\n",
      "\n",
      "3. **Write code:**  Write Python code using appropriate libraries (like `requests` for making HTTP requests) to interact with the chosen API.  The code will need to construct the search query based on your criteria (CRISPR, genetic disorders, clinical trials, safety protocols, publication dates), send the query to the API, and process the returned results.\n",
      "\n",
      "4. **Process results:** The API response will likely be in JSON format.  Your code will need to parse this JSON data to extract relevant information (titles, abstracts, URLs, etc.).\n",
      "\n",
      "5. **Download papers (optional):** If the API provides URLs to full-text papers, you can use additional code (e.g., using the `requests` library again) to download the papers.  Be mindful of copyright and terms of service.\n",
      "\n",
      "6. **Analyze results (manual):**  Even with automated retrieval, you'll likely need to manually review the abstracts or full texts of the papers to determine their relevance to your specific research question.\n",
      "\n",
      "\n",
      "I can help you with the code if you provide the API documentation for a specific API you want to use.  I can then help you construct the appropriate requests and parse the JSON responses.  However, I cannot directly execute code or access external websites.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## New research running"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Input:\n",
       "\n",
       "Find and analyze papers from 2023-2024 about the application of transformer architectures in protein folding prediction, \n",
       "    specifically looking for novel architectural modifications with experimental validation.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Stream:\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "## Step-by-Step Plan for Research on Transformer Architectures in Protein Folding Prediction\n",
      "\n",
      "This plan outlines the steps to find and analyze papers on transformer architectures in protein folding prediction, focusing on novel architectural modifications with experimental validation.  Each step specifies the tool used and the input parameters.\n",
      "\n",
      "**Step 1: Literature Search for Relevant Papers**\n",
      "\n",
      "* **Tool:** `search-papers`\n",
      "* **Input:**\n",
      "    ```json\n",
      "    {\n",
      "      \"query\": \"transformer architecture protein folding prediction experimental validation\",\n",
      "      \"max_papers\": 10 \n",
      "    }\n",
      "    ```\n",
      "* **Output:** A list of papers matching the query, including titles, abstracts, and URLs.  We'll limit to 10 initially to manage the volume of results.  This query is broad to capture a wide range of relevant papers.\n",
      "\n",
      "**Step 2: Filter Papers by Publication Year and Relevance**\n",
      "\n",
      "* **Tool:** No external tool needed. This step involves manual review of the results from Step 1.\n",
      "* **Process:**  Iterate through the list of papers from Step 1.  Filter out papers not published between 2023 and 2024.  Further filter to include only papers that explicitly mention novel architectural modifications to transformer models *and* experimental validation (e.g., comparison to experimental data, in vitro or in vivo validation).  Record the URLs of the selected papers.\n",
      "\n",
      "**Step 3: Download Selected Papers**\n",
      "\n",
      "* **Tool:** `download-paper`\n",
      "* **Input:** For each URL obtained in Step 2, use the URL as input to this tool.  This will be done iteratively for each relevant paper URL.\n",
      "    ```json\n",
      "    {\n",
      "      \"url\": \"[Paper URL from Step 2]\"\n",
      "    }\n",
      "    ```\n",
      "* **Output:** The full text of each selected paper.\n",
      "\n",
      "**Step 4: Analyze Papers for Novel Architectural Modifications and Validation Methods**\n",
      "\n",
      "* **Tool:** No external tool needed. This is a manual analysis step.\n",
      "* **Process:**  Carefully read each downloaded paper.  Identify and document:\n",
      "    * **Novel Architectural Modifications:**  Specific changes made to the transformer architecture (e.g., new attention mechanisms, novel layer designs, modifications to positional encoding).\n",
      "    * **Experimental Validation Methods:**  The methods used to validate the model's predictions (e.g., comparison to experimental protein structures from databases like PDB, in vitro assays, in vivo experiments).\n",
      "    * **Performance Metrics:**  Quantitative measures of the model's performance (e.g., accuracy, precision, recall, F1-score, RMSD).\n",
      "    * **Limitations:**  Any limitations or weaknesses of the proposed modifications or validation methods.\n",
      "\n",
      "**Step 5: Summarize Findings**\n",
      "\n",
      "* **Tool:** No external tool needed.\n",
      "* **Process:** Create a summary report documenting the findings from Step 4.  This report should include a table summarizing the key characteristics of each paper (publication details, architectural modifications, validation methods, performance metrics, limitations).  The report should also discuss any trends or patterns observed in the analyzed papers.\n",
      "\n",
      "**Step 6: (Optional) Iterative Search Refinement**\n",
      "\n",
      "* **Tool:** `search-papers`\n",
      "* **Process:** If the initial search yielded insufficient results, refine the search query in Step 1 based on the findings from Step 5.  For example, you might add keywords related to specific architectural modifications or validation methods identified in the initial analysis.  Repeat Steps 1-5 with the refined query.\n",
      "\n",
      "\n",
      "This plan provides a structured approach to the research question. The iterative nature of Steps 1-5 allows for refinement and ensures a comprehensive analysis.  The use of external tools automates the data acquisition process, while manual analysis ensures a thorough understanding of the scientific content.\n",
      "\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  search-papers (9be8e670-dcff-4598-b5a6-a14512ac28b1)\n",
      " Call ID: 9be8e670-dcff-4598-b5a6-a14512ac28b1\n",
      "  Args:\n",
      "    max_papers: 10.0\n",
      "    query: transformer architecture protein folding prediction experimental validation AND yearPublished>=2023 AND yearPublished<=2024\n",
      "\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: search-papers\n",
      "\n",
      "\"Error performing paper search: name 'max_retrie' is not defined\"\n",
      "\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The `search_papers` tool encountered an error.  The error message \"name 'max_retrie' is not defined\" suggests a problem within the tool's implementation.  The tool likely expects a parameter named `max_retrie` which is not defined in the provided API documentation.  This is a bug in the `search_papers` tool.\n",
      "\n",
      "To proceed, we need to either:\n",
      "\n",
      "1. **Fix the `search_papers` tool:**  This requires access to the tool's source code to correct the error.  Without access to the source code, this is not possible.\n",
      "\n",
      "2. **Use a different approach:**  We can try to reformulate the query to use only the parameters available in the `search_papers` API.  This might involve a less precise search.  For example, we could remove the year constraint and rely on manual filtering later.\n",
      "\n",
      "Given the limitations, I recommend proceeding with option 2.  I will execute a broader search and then manually filter the results by year and relevance.\n",
      "Tool Calls:\n",
      "  search-papers (d5f37b43-ae52-4e24-b810-88b2201d4966)\n",
      " Call ID: d5f37b43-ae52-4e24-b810-88b2201d4966\n",
      "  Args:\n",
      "    query: transformer architecture protein folding prediction experimental validation\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_inputs = [\n",
    "    \"Download and summarize the findings of this paper: https://pmc.ncbi.nlm.nih.gov/articles/PMC11379842/pdf/11671_2024_Article_4070.pdf\",\n",
    "\n",
    "    \"Can you find 8 papers on quantum machine learning?\",\n",
    "\n",
    "    \"\"\"Find recent papers (2023-2024) about CRISPR applications in treating genetic disorders, \n",
    "    focusing on clinical trials and safety protocols\"\"\",\n",
    "\n",
    "    \"\"\"Find and analyze papers from 2023-2024 about the application of transformer architectures in protein folding prediction, \n",
    "    specifically looking for novel architectural modifications with experimental validation.\"\"\"\n",
    "]\n",
    "\n",
    "# Run tests and store the results for later visualisation\n",
    "outputs = []\n",
    "for test_input in test_inputs:\n",
    "    final_answer = await print_stream(app, test_input)\n",
    "    outputs.append(final_answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2979c11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
